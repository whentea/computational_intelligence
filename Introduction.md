# Computational-Intelligence  

 
  
>>Nature has evolved over billions of years, providing a rich source of inspiration {2, Preface}  
  
Most convensional or classic algorithms are deterministric. Some deterministic optimization algorithms used the gradient information, so they are called gradient-based algorithms.  
It does not work well in some discontinuity in the object function.  
In this case, nongradient-based or gradient-free algorithms do not use any derivative, only the function values.  

The new paradigm is the stochastic algorithms. For the stochastic algorithms, in general there are two types: HEURISTIC and METAHEURISTIC.  
The recent trend tends to name all stochastic algorithms with randomization and local search.  



>>The core principle of all CI-based optimization algorithms, which are better known as metaheuristic algorithms, is a way of trial and error to produce an acceptable solution to a complex problem in a reasonably practical time (Yang 2010) {1, Page 3}

  
**Refer to:**  
[1] Omid Bozorg-Haddad (2018), Advanced Optimization by Nature-Inspired Algorithms (Studies Computational Intelligence), Springer.  
[2] Xin-She Yang (2014), Nature-Inspired Optimization Algorithms, Elsevier Inc.
