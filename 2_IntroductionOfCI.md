# Computational-Intelligence  
  
Most convensional or classic algorithms are deterministric. Some deterministic optimization algorithms used the gradient information, so they are called gradient-based algorithms.  
It does not work well in some discontinuity in the object function.  
In this case, nongradient-based or gradient-free algorithms do not use any derivative, only the function values.  

The new paradigm is the stochastic algorithms. The recent trend tends to name all stochastic algorithms with randomization and local search.  
For the stochastic algorithms, in general there are two types: HEURISTIC and METAHEURISTIC.  
Two major components of any metaheuristic algorithms are INTENSIFICATION and DIVERSIFICATION, or EXPLOITATION and EXPLORATION.  
The good combination of these two major components will usually ensure that the global optimally is achievable.  

Metaheuristic algorithms can be classified in many ways. One way is to classify them as population-based (multiple agents or particles) or trajectory-based, an the other one is a single agent.  

>>The core principle of all CI-based optimization algorithms, which are better known as metaheuristic algorithms, is a way of trial and error to produce an acceptable solution to a complex problem in a reasonably practical time (Yang 2010) {1, Page 3}  
  
  
**Refer to:**  
[1] Omid Bozorg-Haddad (2018), Advanced Optimization by Nature-Inspired Algorithms (Studies Computational Intelligence), Springer.  
[2] Xin-She Yang (2014), Nature-Inspired Optimization Algorithms, Elsevier Inc.
